# Basic configuration
model: "Models.HierarchicalTransformer.HierarchicalTransformer"
dataset: "smartfallmm"

# Model parameters - these are passed directly to the model
model_args:
  window_seq_len: 128          # Sequence length per window
  num_windows: 3               # Number of consecutive windows per sample
  channels: 4                   # 3 accelerometer axes + 1 SMV
  embed_dim: 128
  num_heads: 4
  mlp_dim: 256
  num_layers_window: 2          # Transformer layers for window-level
  num_layers_sequence: 2        # Transformer layers for sequence-level
  dropout_rate: 0.25
  attn_dropout_rate: 0.25
  num_classes: 1                # Binary classification

## Dataset arguments must match the exact structure expected by the dataset class
dataset_args:
  mode: "sliding_window"          # Pooling mode
  max_length: 128                 # Sequence length
  task: "fd"                      # Task: fall detection
  modalities: ["accelerometer"]   # Only accelerometer data
  age_group: ["young"]
  sensors: ["watch"]
  normalize: true                 # Normalize input data
  stride: 32                      # Stride length for sliding window
  num_context_windows: 1          # Number of surrounding windows (on each side)

# Subject IDs as a flat list for the argument parser
subjects:
  - 29
  - 30
  - 31
  - 32
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 43
  - 44
  - 45
  - 46

# Training parameters that match main4.py arguments exactly
batch_size: 32
test_batch_size: 32 
val_batch_size: 32
num_epoch: 150
start_epoch: 0

# Optimizer settings that match the argument parser
optimizer: "adamw"
base_lr: 0.0001
weight_decay: 0.0004

# Device configuration as expected by the script
device: [0]

# Feeder settings that match main4.py's requirements
feeder: "Feeder.Make_Dataset.UTD_mm"
train_feeder_args:
  batch_size: 32
val_feeder_args:
  batch_size: 32
test_feeder_args:
  batch_size: 32

# Additional training settings
num_worker: 4
seed: 42
include_val: true
print_log: true
phase: "train"

# Loss function configuration 
loss: "torch.nn.BCELoss"  # Binary Cross-Entropy Loss
loss_args: {}

# Scheduler settings
scheduler: "cosine"
scheduler_args:
  warmup_epochs: 10
