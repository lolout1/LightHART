{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948e7649-932a-452e-9f6b-4f1eff6da897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:23:17.728622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732159397.776086  275183 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732159397.794692  275183 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 21:23:17.897936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "/tmp/ipykernel_275183/1112376939.py:207: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  add_safe_globals([np.core.multiarray.scalar])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732159415.398424  275183 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptujrlrz3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptujrlrz3/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1732159418.625831  275183 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732159418.625903  275183 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:23:38.627858: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmptujrlrz3\n",
      "2024-11-20 21:23:38.629511: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:23:38.629543: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmptujrlrz3\n",
      "I0000 00:00:1732159418.644897  275183 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2024-11-20 21:23:38.647251: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:23:38.751646: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmptujrlrz3\n",
      "2024-11-20 21:23:38.781135: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 153299 microseconds.\n",
      "2024-11-20 21:23:38.818596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-20 21:23:39.107914: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 11.395 M  ops, equivalently 5.698 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # You can set this to False if nested tensors are required\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Convert the wrapped model to LiteRT\n",
    "    try:\n",
    "        edge_model = ai_edge_torch.convert(wrapped_model, sample_args)\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b734794f-549f-4ecf-8e19-e5a3b2b1b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully exported to 'mobile_falldet.tflite'\n"
     ]
    }
   ],
   "source": [
    "# Export the LiteRT model to TFLite\n",
    "edge_model.export('mobile_falldet2.tflite')\n",
    "print(\"Model successfully exported to 'mobile_falldet.tflite'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a27e13-63e0-4147-a40e-1f3f950c5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:10:47.494061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732158647.519374  271538 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732158647.527195  271538 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 21:10:47.569229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "/tmp/ipykernel_271538/735383770.py:207: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  add_safe_globals([np.core.multiarray.scalar])\n",
      "W1120 21:10:51.581000 271538 torch/_export/__init__.py:64] +============================+\n",
      "W1120 21:10:51.582000 271538 torch/_export/__init__.py:65] |     !!!   WARNING   !!!    |\n",
      "W1120 21:10:51.584000 271538 torch/_export/__init__.py:66] +============================+\n",
      "W1120 21:10:51.585000 271538 torch/_export/__init__.py:67] capture_pre_autograd_graph() is deprecated and doesn't provide any function guarantee moving forward.\n",
      "W1120 21:10:51.587000 271538 torch/_export/__init__.py:68] Please switch to use torch.export.export_for_training instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_1) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_2) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_3) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_4) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_5) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_6) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_7) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n",
      "I0000 00:00:1732158665.497585  271538 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt9ri5xs0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt9ri5xs0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion failed: Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True<unknown>:0: error: loc(callsite(callsite(callsite(\"torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl/__main__.DualPathFallDetector_original_model/__main__.SMVProcessor_phone_smv_processor/torch.nn.modules.container.Sequential_smv_encoder/torch.nn.modules.conv.Conv1d_0;\" at fused[\"XlaCallModule:\", \"XlaCallModule@__inference_inner_572\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_731\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tfl.transpose' op has mismatched quantized axes of input and output\n",
      "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n",
      "\n",
      "Failed to save quantized model: name 'pt2e_drq_model' is not defined\n",
      "Failed to export quantized model to TFLite: name 'pt2e_drq_model' is not defined\n",
      "Validation of quantized model failed: name 'pt2e_drq_model' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1732158668.828573  271538 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732158668.828642  271538 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:11:08.829464: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpt9ri5xs0\n",
      "2024-11-20 21:11:08.831443: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:11:08.831467: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpt9ri5xs0\n",
      "I0000 00:00:1732158668.850352  271538 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2024-11-20 21:11:08.853217: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:11:09.004122: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpt9ri5xs0\n",
      "2024-11-20 21:11:09.039545: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 210085 microseconds.\n",
      "2024-11-20 21:11:09.070943: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "loc(callsite(callsite(callsite(\"torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl/__main__.DualPathFallDetector_original_model/__main__.SMVProcessor_phone_smv_processor/torch.nn.modules.container.Sequential_smv_encoder/torch.nn.modules.conv.Conv1d_0;\" at fused[\"XlaCallModule:\", \"XlaCallModule@__inference_inner_572\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_731\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tfl.transpose' op has mismatched quantized axes of input and output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # You can set this to False if nested tensors are required\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Quantization Steps\n",
    "    from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "    from torch._export import capture_pre_autograd_graph\n",
    "\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import get_symmetric_quantization_config\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import PT2EQuantizer\n",
    "    from ai_edge_torch.quantize.quant_config import QuantConfig\n",
    "\n",
    "    # Initialize the PT2E Quantizer with symmetric quantization configuration\n",
    "    pt2e_quantizer = PT2EQuantizer().set_global(\n",
    "        get_symmetric_quantization_config(is_per_channel=True, is_dynamic=True)\n",
    "    )\n",
    "\n",
    "    # Capture the pre-autograd graph of the wrapped model\n",
    "    pt2e_torch_model = capture_pre_autograd_graph(wrapped_model, sample_args)\n",
    "\n",
    "    # Prepare the model for PT2E quantization\n",
    "    pt2e_torch_model = prepare_pt2e(pt2e_torch_model, pt2e_quantizer)\n",
    "\n",
    "    # Run the prepared model with sample input data to ensure that internal observers are populated with correct values\n",
    "    pt2e_torch_model(*sample_args)\n",
    "\n",
    "    # Convert the prepared model to a quantized model\n",
    "    pt2e_torch_model = convert_pt2e(pt2e_torch_model, fold_quantize=False)\n",
    "\n",
    "    # Convert to an ai_edge_torch model with quantization configuration and additional converter flags\n",
    "    try:\n",
    "        pt2e_drq_model = ai_edge_torch.convert(\n",
    "            pt2e_torch_model,\n",
    "            sample_args,\n",
    "            quant_config=QuantConfig(pt2e_quantizer=pt2e_quantizer),\n",
    "            _ai_edge_converter_flags={'experimental_enable_resource_variables': True}\n",
    "        )\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "        # Depending on the error, consider alternative approaches below\n",
    "\n",
    "    # Save the quantized model with a different name\n",
    "    try:\n",
    "        quantized_model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414_quant.pth\"\n",
    "        torch.save(pt2e_drq_model.state_dict(), quantized_model_path)\n",
    "        print(f\"Quantized model saved successfully at '{quantized_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save quantized model: {e}\")\n",
    "\n",
    "    # Optional: Convert the quantized model to TFLite\n",
    "    try:\n",
    "        # Export the quantized LiteRT model to TFLite\n",
    "        pt2e_drq_model.export('mobile_falldet_quant.tflite')\n",
    "        print(\"Quantized model successfully exported to 'mobile_falldet_quant.tflite'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export quantized model to TFLite: {e}\")\n",
    "\n",
    "    # Optional: Validate the Quantized Model\n",
    "    try:\n",
    "        # Perform inference with the quantized PyTorch model\n",
    "        with torch.no_grad():\n",
    "            torch_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Perform inference with the quantized LiteRT model\n",
    "        # Assuming that pt2e_drq_model can perform inference like this\n",
    "        # If not, you may need to load the TFLite model separately for inference\n",
    "        tfl_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Extract logits and SMV features\n",
    "        torch_quant_logits = torch_quant_output[0].detach().numpy()\n",
    "        tfl_quant_logits = tfl_quant_output[0]\n",
    "\n",
    "        torch_quant_smv_features = {k: v.detach().numpy() for k, v in torch_quant_output[1].items()}\n",
    "        tfl_quant_smv_features = {k: v for k, v in tfl_quant_output[1].items()}\n",
    "\n",
    "        # Compare logits\n",
    "        if np.allclose(torch_quant_logits, tfl_quant_logits, atol=1e-5, rtol=1e-5):\n",
    "            print(\"Quantized inference result for logits with PyTorch and LiteRT matches within tolerance.\")\n",
    "        else:\n",
    "            print(\"Discrepancy found in quantized logits between PyTorch and LiteRT models.\")\n",
    "\n",
    "        # Compare SMV features\n",
    "        for key in torch_quant_smv_features:\n",
    "            if np.allclose(torch_quant_smv_features[key], tfl_quant_smv_features[key], atol=1e-5, rtol=1e-5):\n",
    "                print(f\"Quantized inference result for {key} matches within tolerance.\")\n",
    "            else:\n",
    "                print(f\"Discrepancy found in quantized {key} between PyTorch and LiteRT models.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation of quantized model failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f62090-2e52-4a8f-993a-176d3aa4d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/abheekp/kd/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/abheekp/kd/lib/python3.10/site-packages (0.20.1)\n",
      "Collecting ai-edge-torch\n",
      "  Downloading ai_edge_torch-0.2.1-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==3.1.0 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: networkx in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: fsspec in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: filelock in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/abheekp/kd/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/abheekp/kd/lib/python3.10/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/abheekp/kd/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Collecting ai-edge-quantizer-nightly==0.0.1.dev20240718\n",
      "  Using cached ai_edge_quantizer_nightly-0.0.1.dev20240718-py3-none-any.whl (100 kB)\n",
      "Collecting torch-xla<2.6,>=2.4.0\n",
      "  Downloading torch_xla-2.5.1-cp310-cp310-manylinux_2_28_x86_64.whl (90.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tf-nightly>=2.18.0.dev20240722\n",
      "  Downloading tf_nightly-2.19.0.dev20241119-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (631.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.2/631.2 MB\u001b[0m \u001b[31m425.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m33.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/abheekp/kd/lib/python3.10/site-packages (from ai-edge-torch) (1.10.1)\n",
      "Collecting safetensors\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Collecting immutabledict\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: packaging in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Collecting h5py>=3.11.0\n",
      "  Using cached h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Collecting keras>=3.5.0\n",
      "  Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.3/381.3 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Collecting keras-nightly>=3.6.0.dev\n",
      "  Downloading keras_nightly-3.7.0.dev2024112003-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tb-nightly~=2.19.0.a\n",
      "  Downloading tb_nightly-2.19.0a20241120-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/abheekp/kd/lib/python3.10/site-packages (from torch-xla<2.6,>=2.4.0->ai-edge-torch) (6.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abheekp/kd/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/abheekp/kd/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, tabulate, safetensors, optree, opt-einsum, numpy, mdurl, markdown, immutabledict, grpcio, google-pasta, gast, absl-py, torch-xla, tensorboard, tb-nightly, ml-dtypes, markdown-it-py, h5py, astunparse, rich, keras-nightly, keras, tf-nightly, tensorflow, ai-edge-quantizer-nightly, ai-edge-torch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed absl-py-2.1.0 ai-edge-quantizer-nightly-0.0.1.dev20240718 ai-edge-torch-0.2.1 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 h5py-3.12.1 immutabledict-4.2.1 keras-3.6.0 keras-nightly-3.7.0.dev2024112003 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.4.0 optree-0.13.1 rich-13.9.4 safetensors-0.4.5 tabulate-0.9.0 tb-nightly-2.19.0a20241120 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 tf-nightly-2.19.0.dev20241119 torch-xla-2.5.1 werkzeug-3.1.3 wheel-0.45.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision ai-edge-torch tensorflow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd74f4b6-5e9d-4306-9a34-d2fcaaaf9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_1) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_2) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_3) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_4) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_5) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_6) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_7) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0rx_dv7s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0rx_dv7s/assets\n",
      "W0000 00:00:1732158995.136254  271538 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732158995.136791  271538 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:16:35.138567: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp0rx_dv7s\n",
      "2024-11-20 21:16:35.140593: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:16:35.140633: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp0rx_dv7s\n",
      "2024-11-20 21:16:35.156286: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:16:35.275884: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp0rx_dv7s\n",
      "2024-11-20 21:16:35.299945: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 161527 microseconds.\n",
      "2024-11-20 21:16:35.755475: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 11.297 M  ops, equivalently 5.648 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion successful!\n",
      "Failed to save quantized model: 'TfLiteModel' object has no attribute 'state_dict'\n",
      "Quantized model successfully exported to 'mobile_falldet_quant.tflite'.\n",
      "Validation of quantized model failed: 'numpy.ndarray' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress specific UserWarnings from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Can be set to False if needed\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Quantization Steps\n",
    "    from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "    from torch._export import capture_pre_autograd_graph\n",
    "\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import get_symmetric_quantization_config\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import PT2EQuantizer\n",
    "    from ai_edge_torch.quantize.quant_config import QuantConfig\n",
    "\n",
    "    # Initialize the PT2E Quantizer with symmetric quantization configuration (per-tensor and dynamic)\n",
    "    pt2e_quantizer = PT2EQuantizer().set_global(\n",
    "        get_symmetric_quantization_config(is_per_channel=False, is_dynamic=True)\n",
    "    )\n",
    "\n",
    "    # Capture the pre-autograd graph of the wrapped model\n",
    "    pt2e_torch_model = capture_pre_autograd_graph(wrapped_model, sample_args)\n",
    "\n",
    "    # Prepare the model for PT2E quantization\n",
    "    pt2e_torch_model = prepare_pt2e(pt2e_torch_model, pt2e_quantizer)\n",
    "\n",
    "    # Run the prepared model with sample input data to ensure that internal observers are populated with correct values\n",
    "    pt2e_torch_model(*sample_args)\n",
    "\n",
    "    # Convert the prepared model to a quantized model\n",
    "    pt2e_torch_model = convert_pt2e(pt2e_torch_model, fold_quantize=False)\n",
    "\n",
    "    # Conversion Flags\n",
    "    _ai_edge_converter_flags = {\n",
    "        'experimental_enable_resource_variables': True\n",
    "    }\n",
    "\n",
    "    # Convert to an ai_edge_torch model with quantization configuration and additional converter flags\n",
    "    try:\n",
    "        pt2e_drq_model = ai_edge_torch.convert(\n",
    "            pt2e_torch_model,\n",
    "            sample_args,\n",
    "            quant_config=QuantConfig(pt2e_quantizer=pt2e_quantizer),\n",
    "            _ai_edge_converter_flags=_ai_edge_converter_flags\n",
    "        )\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "        # Depending on the error, consider alternative approaches below\n",
    "\n",
    "    # Save the quantized model with a different name\n",
    "    try:\n",
    "        quantized_model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414_quant.pth\"\n",
    "        torch.save(pt2e_drq_model.state_dict(), quantized_model_path)\n",
    "        print(f\"Quantized model saved successfully at '{quantized_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save quantized model: {e}\")\n",
    "\n",
    "    # Optional: Convert the quantized model to TFLite\n",
    "    try:\n",
    "        # Export the quantized LiteRT model to TFLite\n",
    "        pt2e_drq_model.export('mobile_falldet_quant.tflite')\n",
    "        print(\"Quantized model successfully exported to 'mobile_falldet_quant.tflite'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export quantized model to TFLite: {e}\")\n",
    "\n",
    "    # Optional: Validate the Quantized Model\n",
    "    try:\n",
    "        # Perform inference with the quantized PyTorch model\n",
    "        with torch.no_grad():\n",
    "            torch_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Perform inference with the quantized LiteRT model\n",
    "        # Note: AI Edge Torch's LiteRT model might require a different inference approach.\n",
    "        # Here, we assume it can be invoked similarly.\n",
    "        # If not, you may need to load the TFLite model separately for inference.\n",
    "        tfl_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Extract logits and SMV features\n",
    "        torch_quant_logits = torch_quant_output[0].detach().numpy()\n",
    "        tfl_quant_logits = tfl_quant_output[0]\n",
    "\n",
    "        torch_quant_smv_features = {k: v.detach().numpy() for k, v in torch_quant_output[1].items()}\n",
    "        tfl_quant_smv_features = {k: v for k, v in tfl_quant_output[1].items()}\n",
    "\n",
    "        # Compare logits\n",
    "        if np.allclose(torch_quant_logits, tfl_quant_logits, atol=1e-5, rtol=1e-5):\n",
    "            print(\"Quantized inference result for logits with PyTorch and LiteRT matches within tolerance.\")\n",
    "        else:\n",
    "            print(\"Discrepancy found in quantized logits between PyTorch and LiteRT models.\")\n",
    "\n",
    "        # Compare SMV features\n",
    "        for key in torch_quant_smv_features:\n",
    "            if np.allclose(torch_quant_smv_features[key], tfl_quant_smv_features[key], atol=1e-5, rtol=1e-5):\n",
    "                print(f\"Quantized inference result for {key} matches within tolerance.\")\n",
    "            else:\n",
    "                print(f\"Discrepancy found in quantized {key} between PyTorch and LiteRT models.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation of quantized model failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416ffc0e-1f66-4eb0-b3ee-273f8940a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details:\n",
      "{'name': 'serving_default_args_0:0', 'index': 0, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'serving_default_args_1:0', 'index': 1, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "Output Details:\n",
      "{'name': 'StatefulPartitionedCall:1', 'index': 407, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 398, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:2', 'index': 414, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "Logits: [0.42323497]\n",
      "SMV Features: [[-2.19007    1.9581431]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "`dim` must be in the range [-1, 1) where 1 is the number of dimensions in the input. Received: dim=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMV Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, smv_features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Post-processing: Apply softmax to logits to get probabilities\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProbabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, probabilities)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Determine the predicted class\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/tensorflow/python/ops/nn_ops.py:3836\u001b[0m, in \u001b[0;36m_wrap_2d_function\u001b[0;34m(inputs, compute_op, dim, name)\u001b[0m\n\u001b[1;32m   3834\u001b[0m   dim_val \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconstant_value(dim)\n\u001b[1;32m   3835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m-\u001b[39mshape\u001b[38;5;241m.\u001b[39mndims \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dim_val \u001b[38;5;241m<\u001b[39m shape\u001b[38;5;241m.\u001b[39mndims:\n\u001b[0;32m-> 3836\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m   3837\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3838\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dim` must be in the range [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m-\u001b[39mshape\u001b[38;5;241m.\u001b[39mndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;241m.\u001b[39mndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) where \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3839\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;241m.\u001b[39mndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is the number of dimensions in the input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3840\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3842\u001b[0m \u001b[38;5;66;03m# If dim is not the last dimension, we have to do a transpose so that we can\u001b[39;00m\n\u001b[1;32m   3843\u001b[0m \u001b[38;5;66;03m# still perform the op on its last dimension.\u001b[39;00m\n\u001b[1;32m   3844\u001b[0m \n\u001b[1;32m   3845\u001b[0m \u001b[38;5;66;03m# In case dim is negative (and is not last dimension -1), add shape.ndims\u001b[39;00m\n\u001b[1;32m   3846\u001b[0m ndims \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mrank(inputs)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: `dim` must be in the range [-1, 1) where 1 is the number of dimensions in the input. Received: dim=1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your TFLite model\n",
    "tflite_model_path = \"mobile_falldet_quant.tflite\"\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\")\n",
    "for detail in input_details:\n",
    "    print(detail)\n",
    "\n",
    "print(\"\\nOutput Details:\")\n",
    "for detail in output_details:\n",
    "    print(detail)\n",
    "\n",
    "# Prepare sample input data\n",
    "# Replace these with actual sensor data in practice\n",
    "phone_input = np.random.rand(1, 128, 4).astype(np.float32)    # Shape: [1, 128, 4]\n",
    "watch_input = np.random.rand(1, 128, 4).astype(np.float32)   # Shape: [1, 128, 4]\n",
    "\n",
    "# Set tensor for 'accelerometer_phone'\n",
    "interpreter.set_tensor(input_details[0]['index'], phone_input)\n",
    "\n",
    "# Set tensor for 'accelerometer_watch'\n",
    "interpreter.set_tensor(input_details[1]['index'], watch_input)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensors\n",
    "logits = interpreter.get_tensor(output_details[0]['index'])\n",
    "smv_features = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "print(\"\\nLogits:\", logits)\n",
    "print(\"SMV Features:\", smv_features)\n",
    "\n",
    "# Post-processing: Apply softmax to logits to get probabilities\n",
    "probabilities = tf.nn.softmax(logits, axis=1).numpy()\n",
    "print(\"\\nProbabilities:\", probabilities)\n",
    "\n",
    "# Determine the predicted class\n",
    "predicted_class = np.argmax(probabilities, axis=1)\n",
    "print(\"Predicted Class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edec4e2f-067b-47cc-89f6-8b72e82d06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details:\n",
      "{'name': 'serving_default_args_0:0', 'index': 0, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'serving_default_args_1:0', 'index': 1, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "Output Details:\n",
      "{'name': 'StatefulPartitionedCall:1', 'index': 466, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 460, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:2', 'index': 472, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Processing Output Tensor: StatefulPartitionedCall:1, Shape: [1], Data: [0.01611325]\n",
      "Assigned 'StatefulPartitionedCall:1' to smv_features_1.\n",
      "Processing Output Tensor: StatefulPartitionedCall:0, Shape: [1 2], Data: [[ 1.5169027 -1.52473  ]]\n",
      "Assigned 'StatefulPartitionedCall:0' to logits.\n",
      "Processing Output Tensor: StatefulPartitionedCall:2, Shape: [1], Data: [0.01524284]\n",
      "Assigned 'StatefulPartitionedCall:2' to smv_features_2.\n",
      "\n",
      "Logits: [[ 1.5169027 -1.52473  ]]\n",
      "SMV Features 1: [0.01611325]\n",
      "SMV Features 2: [0.01524284]\n",
      "\n",
      "Probabilities: [[0.95441985 0.04558009]]\n",
      "Predicted Class: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your TFLite model\n",
    "tflite_model_path = \"mobile_falldet2.tflite\"\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\")\n",
    "for detail in input_details:\n",
    "    print(detail)\n",
    "\n",
    "print(\"\\nOutput Details:\")\n",
    "for detail in output_details:\n",
    "    print(detail)\n",
    "\n",
    "# Prepare sample input data\n",
    "# Replace these with actual sensor data in practice\n",
    "phone_input = np.random.rand(1, 128, 4).astype(np.float32)    # Shape: [1, 128, 4]\n",
    "watch_input = np.random.rand(1, 128, 4).astype(np.float32)   # Shape: [1, 128, 4]\n",
    "\n",
    "# Set tensor for 'accelerometer_phone'\n",
    "interpreter.set_tensor(input_details[0]['index'], phone_input)\n",
    "\n",
    "# Set tensor for 'accelerometer_watch'\n",
    "interpreter.set_tensor(input_details[1]['index'], watch_input)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Assign outputs based on name and shape\n",
    "logits = None\n",
    "smv_features_1 = None\n",
    "smv_features_2 = None\n",
    "\n",
    "for detail in output_details:\n",
    "    tensor = interpreter.get_tensor(detail['index'])\n",
    "    name = detail['name']\n",
    "    shape = detail['shape']\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(f\"Processing Output Tensor: {name}, Shape: {shape}, Data: {tensor}\")\n",
    "\n",
    "    # Correct shape comparison using np.array_equal\n",
    "    if name == 'StatefulPartitionedCall:0' and np.array_equal(shape, [1, 2]):\n",
    "        logits = tensor\n",
    "        print(f\"Assigned '{name}' to logits.\")\n",
    "    elif name == 'StatefulPartitionedCall:1' and np.array_equal(shape, [1]):\n",
    "        smv_features_1 = tensor\n",
    "        print(f\"Assigned '{name}' to smv_features_1.\")\n",
    "    elif name == 'StatefulPartitionedCall:2' and np.array_equal(shape, [1]):\n",
    "        smv_features_2 = tensor\n",
    "        print(f\"Assigned '{name}' to smv_features_2.\")\n",
    "    else:\n",
    "        print(f\"Unrecognized tensor: {name} with shape: {shape}\")\n",
    "\n",
    "# Verify that logits have been correctly assigned\n",
    "if logits is None:\n",
    "    raise ValueError(\"Logits tensor not found in the model outputs.\")\n",
    "if smv_features_1 is None or smv_features_2 is None:\n",
    "    print(\"Warning: One or more SMV features were not found in the model outputs.\")\n",
    "\n",
    "print(\"\\nLogits:\", logits)\n",
    "print(\"SMV Features 1:\", smv_features_1)\n",
    "print(\"SMV Features 2:\", smv_features_2)\n",
    "\n",
    "# Post-processing: Apply softmax to logits to get probabilities\n",
    "# Ensure that logits have shape [1, 2] before applying softmax\n",
    "if logits.ndim == 2 and logits.shape[1] == 2:\n",
    "    probabilities = tf.nn.softmax(logits, axis=1).numpy()\n",
    "    print(\"\\nProbabilities:\", probabilities)\n",
    "    \n",
    "    # Determine the predicted class\n",
    "    predicted_class = np.argmax(probabilities, axis=1)\n",
    "    print(\"Predicted Class:\", predicted_class)\n",
    "else:\n",
    "    print(\"Logits tensor has an unexpected shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac264bd-fbbf-4f77-a529-85f767901ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:30:06.809417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732159806.835889  277252 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732159806.844495  277252 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 21:30:06.892289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "/tmp/ipykernel_277252/4071810581.py:216: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  add_safe_globals([np.core.multiarray.scalar])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732159823.564281  277252 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpay1u275n/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpay1u275n/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1732159826.296060  277252 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732159826.296149  277252 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:30:26.296846: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpay1u275n\n",
      "2024-11-20 21:30:26.298441: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:30:26.298464: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpay1u275n\n",
      "I0000 00:00:1732159826.314439  277252 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2024-11-20 21:30:26.316565: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:30:26.414157: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpay1u275n\n",
      "2024-11-20 21:30:26.441781: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 144938 microseconds.\n",
      "2024-11-20 21:30:26.469479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-20 21:30:26.740856: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 11.395 M  ops, equivalently 5.698 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion successful!\n",
      "TFLite model saved successfully at 'mobile_falldet2.tflite'.\n",
      "\n",
      "PyTorch Model Outputs:\n",
      "Logits: [[ 1.4914067 -1.5174068]]\n",
      "SMV Features: {'phone_smv': array([0.01581427], dtype=float32), 'watch_smv': array([0.01431987], dtype=float32)}\n",
      "Processing Output Tensor: StatefulPartitionedCall:1, Shape: [1], Data: [0.01581427]\n",
      "Assigned 'StatefulPartitionedCall:1' to smv_features_1.\n",
      "Processing Output Tensor: StatefulPartitionedCall:0, Shape: [1 2], Data: [[ 1.4914067 -1.517407 ]]\n",
      "Assigned 'StatefulPartitionedCall:0' to logits.\n",
      "Processing Output Tensor: StatefulPartitionedCall:2, Shape: [1], Data: [0.01431986]\n",
      "Assigned 'StatefulPartitionedCall:2' to smv_features_2.\n",
      "\n",
      "TFLite Model Outputs:\n",
      "Logits: [[ 1.4914067 -1.517407 ]]\n",
      "SMV Features: {'smv_features_1': array([0.01581427], dtype=float32), 'smv_features_2': array([0.01431986], dtype=float32)}\n",
      "\n",
      "Logits Comparison:\n",
      "Absolute Differences: [[0.0000000e+00 1.1920929e-07]]\n",
      "Mean Squared Error (MSE): 7.105427357601002e-15\n",
      "Mean Absolute Error (MAE): 5.960464477539063e-08\n",
      "\n",
      "SMV Feature 'phone_smv' not found in TFLite outputs.\n",
      "\n",
      "SMV Feature 'watch_smv' not found in TFLite outputs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define the PyTorch Model\n",
    "# ------------------------------\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # You can set this to False if nested tensors are required\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load and Convert the PyTorch Model\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Convert the wrapped model to LiteRT\n",
    "    try:\n",
    "        edge_model = ai_edge_torch.convert(wrapped_model, sample_args)\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Save the TFLite Model\n",
    "# ------------------------------\n",
    "\n",
    "# Assuming the conversion was successful, save the TFLite model\n",
    "if 'edge_model' in locals():\n",
    "    try:\n",
    "        tflite_model_path = \"mobile_falldet2.tflite\"\n",
    "        edge_model.export(tflite_model_path)\n",
    "        print(f\"TFLite model saved successfully at '{tflite_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save TFLite model: {e}\")\n",
    "else:\n",
    "    print(\"Edge model not found. Cannot export to TFLite.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Perform Inference and Compare Outputs\n",
    "# ------------------------------\n",
    "\n",
    "# Function to run PyTorch inference\n",
    "def run_pytorch_inference(model, phone_input, watch_input):\n",
    "    \"\"\"\n",
    "    Runs inference on the PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The wrapped PyTorch model.\n",
    "        phone_input (torch.Tensor): Tensor for accelerometer_phone.\n",
    "        watch_input (torch.Tensor): Tensor for accelerometer_watch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: logits as NumPy array and SMV features as a dictionary.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, smv_features = model(phone_input, watch_input)\n",
    "    return logits.numpy(), {k: v.numpy() for k, v in smv_features.items()}\n",
    "\n",
    "# Function to run TFLite inference\n",
    "def run_tflite_inference(tflite_model_path, phone_input, watch_input):\n",
    "    \"\"\"\n",
    "    Runs inference on the TFLite model.\n",
    "\n",
    "    Args:\n",
    "        tflite_model_path (str): Path to the TFLite model.\n",
    "        phone_input (np.ndarray): Input array for accelerometer_phone.\n",
    "        watch_input (np.ndarray): Input array for accelerometer_watch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: logits as NumPy array and SMV features as a dictionary.\n",
    "    \"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensor details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Ensure the inputs are in the correct order\n",
    "    # Assuming 'serving_default_args_0:0' corresponds to 'accelerometer_phone'\n",
    "    # and 'serving_default_args_1:0' corresponds to 'accelerometer_watch'\n",
    "    interpreter.set_tensor(input_details[0]['index'], phone_input)\n",
    "    interpreter.set_tensor(input_details[1]['index'], watch_input)\n",
    "\n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Assign outputs based on name and shape\n",
    "    logits = None\n",
    "    smv_features_1 = None\n",
    "    smv_features_2 = None\n",
    "\n",
    "    for detail in output_details:\n",
    "        tensor = interpreter.get_tensor(detail['index'])\n",
    "        name = detail['name']\n",
    "        shape = detail['shape']\n",
    "        \n",
    "        # Debugging prints\n",
    "        print(f\"Processing Output Tensor: {name}, Shape: {shape}, Data: {tensor}\")\n",
    "\n",
    "        # Correct shape comparison using np.array_equal\n",
    "        if name == 'StatefulPartitionedCall:0' and np.array_equal(shape, [1, 2]):\n",
    "            logits = tensor\n",
    "            print(f\"Assigned '{name}' to logits.\")\n",
    "        elif name == 'StatefulPartitionedCall:1' and np.array_equal(shape, [1]):\n",
    "            smv_features_1 = tensor\n",
    "            print(f\"Assigned '{name}' to smv_features_1.\")\n",
    "        elif name == 'StatefulPartitionedCall:2' and np.array_equal(shape, [1]):\n",
    "            smv_features_2 = tensor\n",
    "            print(f\"Assigned '{name}' to smv_features_2.\")\n",
    "        else:\n",
    "            print(f\"Unrecognized tensor: {name} with shape: {shape}\")\n",
    "\n",
    "    # Verify that logits have been correctly assigned\n",
    "    if logits is None:\n",
    "        raise ValueError(\"Logits tensor not found in the model outputs.\")\n",
    "    if smv_features_1 is None or smv_features_2 is None:\n",
    "        print(\"Warning: One or more SMV features were not found in the model outputs.\")\n",
    "\n",
    "    return logits, {'smv_features_1': smv_features_1, 'smv_features_2': smv_features_2}\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Generate Consistent Random Input Data\n",
    "# ------------------------------\n",
    "\n",
    "# For reproducibility, set the random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate deterministic random input data\n",
    "phone_input_np = np.random.rand(1, 128, 4).astype(np.float32)    # Shape: [1, 128, 4]\n",
    "watch_input_np = np.random.rand(1, 128, 4).astype(np.float32)   # Shape: [1, 128, 4]\n",
    "\n",
    "phone_input_torch = torch.from_numpy(phone_input_np)\n",
    "watch_input_torch = torch.from_numpy(watch_input_np)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Run Inference on Both Models\n",
    "# ------------------------------\n",
    "\n",
    "# Run PyTorch inference\n",
    "if 'wrapped_model' in locals():\n",
    "    pytorch_logits, pytorch_smv = run_pytorch_inference(wrapped_model, phone_input_torch, watch_input_torch)\n",
    "    print(\"\\nPyTorch Model Outputs:\")\n",
    "    print(\"Logits:\", pytorch_logits)\n",
    "    print(\"SMV Features:\", pytorch_smv)\n",
    "else:\n",
    "    print(\"Wrapped model not found. Skipping PyTorch inference.\")\n",
    "\n",
    "# Run TFLite inference\n",
    "if os.path.exists(\"mobile_falldet2.tflite\"):\n",
    "    tflite_logits, tflite_smv = run_tflite_inference(\"mobile_falldet2.tflite\", phone_input_np, watch_input_np)\n",
    "    print(\"\\nTFLite Model Outputs:\")\n",
    "    print(\"Logits:\", tflite_logits)\n",
    "    print(\"SMV Features:\", tflite_smv)\n",
    "else:\n",
    "    print(\"TFLite model file 'mobile_falldet2.tflite' not found. Skipping TFLite inference.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Compare the Outputs\n",
    "# ------------------------------\n",
    "\n",
    "if 'pytorch_logits' in locals() and 'tflite_logits' in locals():\n",
    "    # Compare logits\n",
    "    logits_diff = np.abs(pytorch_logits - tflite_logits)\n",
    "    logits_mse = np.mean((pytorch_logits - tflite_logits) ** 2)\n",
    "    logits_mae = np.mean(logits_diff)\n",
    "    \n",
    "    print(\"\\nLogits Comparison:\")\n",
    "    print(\"Absolute Differences:\", logits_diff)\n",
    "    print(f\"Mean Squared Error (MSE): {logits_mse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {logits_mae}\")\n",
    "\n",
    "    # Compare SMV features\n",
    "    for key in pytorch_smv:\n",
    "        pytorch_smv_feat = pytorch_smv[key]\n",
    "        tflite_smv_feat = tflite_smv.get(key, None)\n",
    "        \n",
    "        if tflite_smv_feat is not None:\n",
    "            smv_diff = np.abs(pytorch_smv_feat - tflite_smv_feat)\n",
    "            smv_mse = np.mean((pytorch_smv_feat - tflite_smv_feat) ** 2)\n",
    "            smv_mae = np.mean(smv_diff)\n",
    "            \n",
    "            print(f\"\\nSMV Feature '{key}' Comparison:\")\n",
    "            print(\"Absolute Differences:\", smv_diff)\n",
    "            print(f\"Mean Squared Error (MSE): {smv_mse}\")\n",
    "            print(f\"Mean Absolute Error (MAE): {smv_mae}\")\n",
    "        else:\n",
    "            print(f\"\\nSMV Feature '{key}' not found in TFLite outputs.\")\n",
    "else:\n",
    "    print(\"Insufficient outputs to perform comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973d4247-9a9b-4f5c-8508-1c9451b78346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.. Trying with weights_only=False.\n"
     ]
    },
    {
     "ename": "InternalTorchDynamoError",
     "evalue": "AttributeError: 'NoneType' object has no attribute 'float'\n\nfrom user code:\n   File \"/tmp/ipykernel_279122/54379896.py\", line 167, in forward\n    logits, smv_features = self.original_model(data)\n  File \"/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_279122/54379896.py\", line 140, in forward\n    watch_features, watch_threshold = self.process_device_data(data['accelerometer_watch'].float())\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalTorchDynamoError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 212\u001b[0m\n\u001b[1;32m    205\u001b[0m phone_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    206\u001b[0m watch_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    208\u001b[0m edge_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    209\u001b[0m     \u001b[43mai_edge_torch\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mphone_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatch_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m tflite_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_mobile_falldet.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m edge_model\u001b[38;5;241m.\u001b[39mexport(tflite_model_path)\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/ai_edge_torch/convert/converter.py:134\u001b[0m, in \u001b[0;36mConverter.convert\u001b[0;34m(self, module, sample_args, sample_kwargs, quant_config, dynamic_shapes, _ai_edge_converter_flags)\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# module is provided but not args\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_args or sample_kwargs must be provided if a module is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_signatures\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_signatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_tfl_converter_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ai_edge_converter_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/ai_edge_torch/convert/conversion.py:90\u001b[0m, in \u001b[0;36mconvert_signatures\u001b[0;34m(signatures, quant_config, _tfl_converter_flags)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a list of `Signature`s and embeds them into one `model.TfLiteModel`.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    signatures: The list of 'Signature' objects containing PyTorch modules to be converted.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    quant_config: User-defined quantization method and scheme of the model.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    _tfl_converter_flags: A nested dictionary allowing setting flags for the underlying tflite converter.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m _warn_training_modules(signatures)\n\u001b[0;32m---> 90\u001b[0m exported_programs: torch\u001b[38;5;241m.\u001b[39mexport\u001b[38;5;241m.\u001b[39mExportedProgram \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mexport\u001b[38;5;241m.\u001b[39mexport(sig\u001b[38;5;241m.\u001b[39mmodule, sig\u001b[38;5;241m.\u001b[39mflat_args, dynamic_shapes\u001b[38;5;241m=\u001b[39msig\u001b[38;5;241m.\u001b[39mdynamic_shapes)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m signatures\n\u001b[1;32m     93\u001b[0m ]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Apply default fx passes\u001b[39;00m\n\u001b[1;32m     96\u001b[0m exported_programs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_run_convert_passes, exported_programs))\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/ai_edge_torch/convert/conversion.py:91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a list of `Signature`s and embeds them into one `model.TfLiteModel`.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    signatures: The list of 'Signature' objects containing PyTorch modules to be converted.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    quant_config: User-defined quantization method and scheme of the model.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    _tfl_converter_flags: A nested dictionary allowing setting flags for the underlying tflite converter.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m _warn_training_modules(signatures)\n\u001b[1;32m     90\u001b[0m exported_programs: torch\u001b[38;5;241m.\u001b[39mexport\u001b[38;5;241m.\u001b[39mExportedProgram \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m signatures\n\u001b[1;32m     93\u001b[0m ]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Apply default fx passes\u001b[39;00m\n\u001b[1;32m     96\u001b[0m exported_programs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_run_convert_passes, exported_programs))\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/__init__.py:270\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExporting a ScriptModule is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe try converting your ScriptModule to an ExportedProgram \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing `TS2EPConverter(mod, args, kwargs).convert()` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/_trace.py:1017\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m         log_export_usage(\n\u001b[1;32m   1012\u001b[0m             event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.error.unclassified\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1013\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39merror_type,\n\u001b[1;32m   1014\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m   1015\u001b[0m             flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[1;32m   1016\u001b[0m         )\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1019\u001b[0m     _EXPORT_FLAGS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/_trace.py:990\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 990\u001b[0m     ep \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    992\u001b[0m     log_export_usage(\n\u001b[1;32m    993\u001b[0m         event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    994\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mend \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m    995\u001b[0m         flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_ep_stats(ep),\n\u001b[1;32m    997\u001b[0m     )\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/exported_program.py:114\u001b[0m, in \u001b[0;36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/_trace.py:1880\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;66;03m# Call the appropriate export function based on the strictness of tracing.\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m export_func \u001b[38;5;241m=\u001b[39m _strict_export \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;28;01melse\u001b[39;00m _non_strict_export\n\u001b[0;32m-> 1880\u001b[0m export_artifact \u001b[38;5;241m=\u001b[39m \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[operator]\u001b[39;49;00m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1892\u001b[0m export_graph_signature: ExportGraphSignature \u001b[38;5;241m=\u001b[39m export_artifact\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39msig\n\u001b[1;32m   1894\u001b[0m forward_arg_names \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1895\u001b[0m     _get_forward_arg_names(mod, args, kwargs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_torch_jit_trace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m )\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/_trace.py:1224\u001b[0m, in \u001b[0;36m_strict_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, pre_dispatch, original_state_dict, orig_in_spec, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strict_export\u001b[39m(\n\u001b[1;32m   1212\u001b[0m     mod: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m   1213\u001b[0m     args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     _is_torch_jit_trace: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m   1222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ExportArtifact:\n\u001b[1;32m   1223\u001b[0m     lower_to_aten \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(_export_to_aten_ir, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_strict_export_lower_to_aten_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlower_to_aten_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_to_aten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/_trace.py:1252\u001b[0m, in \u001b[0;36m_strict_export_lower_to_aten_ir\u001b[0;34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, pre_dispatch, original_state_dict, orig_in_spec, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace, lower_to_aten_callback)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strict_export_lower_to_aten_ir\u001b[39m(\n\u001b[1;32m   1240\u001b[0m     mod: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m   1241\u001b[0m     args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     lower_to_aten_callback: Callable,\n\u001b[1;32m   1251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ExportArtifact:\n\u001b[0;32m-> 1252\u001b[0m     gm_torch_level \u001b[38;5;241m=\u001b[39m \u001b[43m_export_to_torch_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestore_fqn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't need to restore because we will do it later\u001b[39;49;00m\n\u001b[1;32m   1259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# We detect the fake_mode by looking at gm_torch_level's placeholders, this is the fake_mode created in dynamo.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     (\n\u001b[1;32m   1265\u001b[0m         fake_args,\n\u001b[1;32m   1266\u001b[0m         fake_kwargs,\n\u001b[1;32m   1267\u001b[0m         dynamo_fake_mode,\n\u001b[1;32m   1268\u001b[0m     ) \u001b[38;5;241m=\u001b[39m _extract_fake_inputs(gm_torch_level, args, kwargs)\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/export/_trace.py:560\u001b[0m, in \u001b[0;36m_export_to_torch_ir\u001b[0;34m(f, args, kwargs, dynamic_shapes, preserve_module_call_signature, disable_constraint_solver, allow_complex_guards_as_runtime_asserts, restore_fqn, _log_export_usage, same_signature)\u001b[0m\n\u001b[1;32m    556\u001b[0m     module_call_specs: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, pytree\u001b[38;5;241m.\u001b[39mTreeSpec]] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _wrap_submodules(\n\u001b[1;32m    558\u001b[0m         f, preserve_module_call_signature, module_call_specs\n\u001b[1;32m    559\u001b[0m     ), _ignore_backend_decomps():\n\u001b[0;32m--> 560\u001b[0m         gm_torch_level, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformed_dynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbolic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_constraint_solver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_constraint_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# currently the following 2 flags are tied together for export purposes,\u001b[39;49;00m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# but untangle for sake of dynamo export api\u001b[39;49;00m\n\u001b[1;32m    567\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_log_export_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43msame_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msame_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ConstraintViolationError, ValueRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(UserErrorType\u001b[38;5;241m.\u001b[39mCONSTRAINT_VIOLATION, \u001b[38;5;28mstr\u001b[39m(e))  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1432\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;66;03m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1432\u001b[0m     result_traced \u001b[38;5;241m=\u001b[39m \u001b[43mopt_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1434\u001b[0m     constraint_violation_error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:952\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;66;03m# Rewrap for clarity\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InternalTorchDynamoError(\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    954\u001b[0m         )\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracer:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1602\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1600\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1601\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1602\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:442\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:836\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3011\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 3011\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3139\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3138\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3139\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3141\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1680\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1680\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:385\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:324\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    321\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:111\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    107\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:836\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3011\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 3011\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3139\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3138\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3139\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3141\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1744\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.LOAD_ATTR\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLOAD_METHOD(inst)\n\u001b[1;32m   1743\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1734\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._load_attr\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_attr\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m   1733\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1734\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mBuiltinVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConstantVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(result)\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:967\u001b[0m, in \u001b[0;36mBuiltinVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handler:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function_handler_cache[key] \u001b[38;5;241m=\u001b[39m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_handler(\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args], \u001b[38;5;28mbool\u001b[39m(kwargs)\n\u001b[1;32m    966\u001b[0m     )\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:848\u001b[0m, in \u001b[0;36mBuiltinVariable._make_handler.<locals>.builtin_dispatch\u001b[0;34m(tx, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuiltin_dispatch\u001b[39m(tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, kwargs):\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m--> 848\u001b[0m         rv \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m rv:\n\u001b[1;32m    850\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:766\u001b[0m, in \u001b[0;36mBuiltinVariable._make_handler.<locals>.call_self_handler\u001b[0;34m(tx, args, kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_self_handler\u001b[39m(tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, kwargs):\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mself_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:1727\u001b[0m, in \u001b[0;36mBuiltinVariable.call_getattr\u001b[0;34m(self, tx, obj, name_var, default)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1716\u001b[0m     obj,\n\u001b[1;32m   1717\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     ),\n\u001b[1;32m   1725\u001b[0m ):\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1727\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_getattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1729\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m GetAttrVariable(obj, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/base.py:244\u001b[0m, in \u001b[0;36mVariableTracker.var_getattr\u001b[0;34m(self, tx, name)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvar_getattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getattr(self, name) returning a new variable\"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconst_getattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mConstantVariable\u001b[38;5;241m.\u001b[39mis_literal(value):\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/_dynamo/variables/constant.py:130\u001b[0m, in \u001b[0;36mConstantVariable.const_getattr\u001b[0;34m(self, tx, name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(\n\u001b[1;32m    125\u001b[0m         UserErrorType\u001b[38;5;241m.\u001b[39mANTI_PATTERN,\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt access members of type(obj) for a generated custom object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use __class__ instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    128\u001b[0m         case_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_reflection_method\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 130\u001b[0m member \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(member):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mInternalTorchDynamoError\u001b[0m: AttributeError: 'NoneType' object has no attribute 'float'\n\nfrom user code:\n   File \"/tmp/ipykernel_279122/54379896.py\", line 167, in forward\n    logits, smv_features = self.original_model(data)\n  File \"/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_279122/54379896.py\", line 140, in forward\n    watch_features, watch_threshold = self.process_device_data(data['accelerometer_watch'].float())\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define the Model Components\n",
    "# ------------------------------\n",
    "\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)\n",
    "        xyz_features = xyz_features.mean(dim=2)\n",
    "        \n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)\n",
    "        \n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)\n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        phone_features, phone_threshold = self.process_device_data(data['accelerometer_phone'].float())\n",
    "        watch_features, watch_threshold = self.process_device_data(data['accelerometer_watch'].float())\n",
    "        \n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Initialize and Load Model\n",
    "# ------------------------------\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"\n",
    "\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}. Trying with weights_only=False.\")\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Convert and Save\n",
    "# ------------------------------\n",
    "os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "phone_sample = torch.randn(1, 128, 4)\n",
    "watch_sample = torch.randn(1, 128, 4)\n",
    "\n",
    "edge_model = (\n",
    "    ai_edge_torch\n",
    "    .signature(\"phone\", wrapped_model, (phone_sample, None))\n",
    "    .signature(\"watch\", wrapped_model, (None, watch_sample))\n",
    "    .convert()\n",
    ")\n",
    "\n",
    "tflite_model_path = \"combined_mobile_falldet.tflite\"\n",
    "edge_model.export(tflite_model_path)\n",
    "print(f\"Combined TFLite model saved at {tflite_model_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d930cd-096a-44c0-99c1-9ec914924daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Edge Torch)",
   "language": "python",
   "name": "ai_edgetorch_convert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
